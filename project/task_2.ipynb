{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen zur Visualisierung der Performance unserer Modelle:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der R²-Wert (Bestimmtheitsmaß) liegt zwischen 0 und 1, wobei 1 bedeutet, dass das Modell die gesamte Variation der abhängigen Variablen erklärt, und 0 bedeutet, dass das Modell keine über die Vorhersage des Durchschnitts hinausgehende Erklärungskraft hat. Ein R²-Wert von 0,5 zeigt beispielsweise, dass 50% der Schwankungen der tatsächlichen Werte durch das Modell erklärt werden können. Dies bedeutet, dass das Modell einen mäßigen Erfolg bei der Erklärung der beobachteten Schwankungen hat. Es gibt jedoch noch Raum für Verbesserungen, da 50 % der Schwankungen nicht durch das Modell erklärt werden.\n",
    "\n",
    "Der MAE steht für \"Mean Absolute Error\" (Durchschnittlicher Absoluter Fehler) und wird verwendet, um die durchschnittliche absolute Abweichung zwischen den tatsächlichen und vorhergesagten Werten zu quantifizieren. Er wird bevorzugt, wenn Ausreißer in den Daten weniger stark gewichtet werden sollen, da der absolute Betrag genommen wird. Je niedriger der MAE, desto besser ist die Modellleistung. Wenn beispielsweise der MAE 0 ist, bedeutet dies, dass das Modell perfekte Vorhersagen getroffen hat. Der MAE ist leicht zu interpretieren, da er angibt, um wie viel Einheiten die durchschnittliche Vorhersage des Modells von den tatsächlichen Werten abweicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measures(y_true, y_pred) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate R2 and MAE\n",
    "    \n",
    "    Args:\n",
    "        y_true: array-like\n",
    "        True values\n",
    "        y_pred: array-like\n",
    "        Predicted values\n",
    "        \n",
    "    Returns:\n",
    "        r2: float\n",
    "        R2 score\n",
    "        mae: float\n",
    "        Mean Absolute Error\n",
    "    \"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"R2: {r2:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    return r2, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_vs_predicted(true_v, pred_v) -> None:\n",
    "    \"\"\"\n",
    "    Plot actual vs. predicted values\n",
    "\n",
    "    Args:\n",
    "        true_v: array-like\n",
    "        True values\n",
    "        pred_v: array-like\n",
    "        Predicted values\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = {\"Actual\": true_v, \"Predicted\": pred_v}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    sns.lineplot(data=df, markers=False)\n",
    "\n",
    "    plt.title(\"Actual vs. Predicted Values\")\n",
    "    plt.xlabel(\"Data Points\")\n",
    "    plt.ylabel(\"Values\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Klasse wird verwendet um eigene Funktionen in die Pipeline zu integrieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"This class is used to apply custom transformations.\"\"\"\n",
    "    def __init__(self, function, config: dict = {}):\n",
    "        self.function = function\n",
    "        self.config = config\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "        X = self.function(X, **self.config) \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function selects the numeric columns from a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        The input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame\n",
    "        A DataFrame with only numeric columns\n",
    "    \"\"\"\n",
    "    df = df.select_dtypes(include=['number'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df: pd.DataFrame, columns: list[str] = []) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops specified columns from a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list of str): List of column names to drop.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with specified columns removed.\n",
    "    \"\"\"\n",
    "    return df.drop(columns=columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Import der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden unsere beiden Datensätze in ```DataFrames```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f\"../data/BikeRentalDaily_train.csv\", sep=\";\", index_col=0)\n",
    "test_data = pd.read_csv(f\"../data/BikeRentalDaily_test.csv\", sep=\";\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ersetzen wir alle Leerzeichen durch `_`. Relevant für `price reduction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns = train_data.columns.str.replace(r\"\\s\", r\"_\", regex=True)\n",
    "test_data.columns = test_data.columns.str.replace(r\"\\s\", r\"_\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Summe der beiden Spalten ```casual``` und ```registered``` ergibt den Wert von ```cnt```, diese sind in der Beschreibung des Datensatzes zudem als Labels beschrieben, daher werden sie ebenfalls entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_data.columns.drop([\"cnt\", \"casual\", \"registered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Minimal Preprocessing and Baseline Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um unsere Modelle unabhängig von den Testdaten valdieren zu können, splitten wir die ```train_data``` Daten in train und validate.\n",
    "\n",
    "Dieser Schritt wird vor jeder erneuten Validierung druchgeführt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir unser Baseline Modell trainieren können, müssen wir Vorverarbeitungsschritte durchführen. Diese werden in der ```minimal_preprocessing``` Pipeline definiert.\n",
    "\n",
    "Zum einen entfernen wir alle nicht numerischen Spalten aus den Daten.\n",
    "\n",
    "Zum anderen werden alle ```NaN``` Werte unter Anwendung des ```SimpleImputer``` durch den jeweiligen Mittlewert aufgefüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_feature = CustomTransformer(function=drop_features, config={\"columns\": [\"dteday\"]})\n",
    "\n",
    "numeric_values = CustomTransformer(function=select_numeric)\n",
    "\n",
    "minimal_pipeline = Pipeline([\n",
    "    (\"drop_feature\", drop_feature),\n",
    "    (\"numeric_values\", numeric_values),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die erste Iteration des Preprocessing erzeugen wir eine ```minimal_pipeline``` welche die beiden zuvor erzeugten Pipelines miteinander kombiniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"minimal_pipeline\", minimal_pipeline)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die einzelnen Schritte der erzeugten Pipeline werden nacheinander ausgeführt und das Modell wird auf dem verarbeiteten Datensatz trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pipeline.fit_transform(train_data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_features = pipeline.named_steps['minimal_pipeline'].named_steps['imputer'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.DataFrame(processed_data, columns=processed_data_features, index=train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(processed_data, train_data[\"cnt\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_01 = Pipeline([(\"baseline_model\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_01.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Anschluss nutzten wir die Validierungsdaten um das Modell damit zu evaluieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model_01.predict(X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die beiden zu Beginn definierten Funktionen geben zum einen die ```performance_measures``` und die Darstellung der ```acutual_vs_predicted``` Werte aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = performance_measures(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus dem Diagramm wird ersichtlich, dass unser ```baseline_model``` aufgrund der Ausreißer im Label ```cnt``` noch Schwierigkeiten bei der Vorhersage hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir bereits in Task 1 erarbeitet haben, weisen die Spalten `season` und `hum` fehlende Werte auf. \n",
    "\n",
    "Im `minimal_preprocessing` werden alle `NaN` Werte mit durch den `SimpleImputer` mit `mean` aufgefüllt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun sollen die fehlenden Werte in der Spalte `season` anhand des vorliegenden Datums in der entsprechenden Zeile aufgefüllt werden. \n",
    "\n",
    "`impute_season` bestimmt die Jahreszeit anhand eines Datums im Format `\"%d.%m.%y\"`. \n",
    "\n",
    "Diese liest den Tag des entsprechenden Datums aus und weist diesen der dafür entsprechenden Season zu.\n",
    "\n",
    "Es gibt vier fest definierte Zeitpunkte für den Beginn jeder Jahreszeit (Frühling, Sommer, Herbst, Winter).\n",
    "\n",
    "Diese Zeitpunkte sind auf den 20. März, 20. Juni, 20. September und 20. Dezember festgelegt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir die entsprechende Season anhand des Datums bestimmen können, müssen wir die Werte der Spalte `dteday` zuvor in `datetime` Objekte umwandeln. \n",
    "\n",
    "Dafür nutzen wir die `feature_to_datetime` Funktion, welche der preprocessing Pipeline als CustomTransformer mitgegeben wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_to_datetime(df: pd.DataFrame, target: str, date_format: str = \"%d.%m.%Y\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the feature to datetime.\n",
    "    \n",
    "    Args:\n",
    "        df: pd.DataFrame: The DataFrame to be transformed.\n",
    "        target: str: The feature to be transformed.\n",
    "        date_format: str: The format of the date.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    df[target] = pd.to_datetime(df[target], format=date_format)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_season(df: pd.DataFrame, target: str, date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute season based on target column value.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame\n",
    "        target (str): target column name\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with season imputed based on target column value\n",
    "    \"\"\"\n",
    "    def get_season(date):\n",
    "        day_of_year = date.timetuple().tm_yday\n",
    "        if 80 <= day_of_year < 172:\n",
    "            return 2\n",
    "        elif 172 <= day_of_year < 265:\n",
    "            return 3\n",
    "        elif 265 <= day_of_year < 355:\n",
    "            return 4\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    if date in df.columns:\n",
    "        df[target] = df[date].apply(get_season)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Zelle definieren wir die einzelnen `CustomTransformer` um die beschriebene Funktionaliät umzusetzten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_datetime = CustomTransformer(function=feature_to_datetime, config={\"target\": \"dteday\"})\n",
    "\n",
    "season_imputer = CustomTransformer(function=impute_season, config={\"target\": \"season\", \"date\": \"dteday\"})\n",
    "\n",
    "missing_value_pipeline = Pipeline([\n",
    "    (\"feature_datetime\", feature_datetime),\n",
    "    (\"season_imputer\", season_imputer)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird die eben definierte Pipeline mit der minimal Pipeline dem ersten Schritt kombiniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"missing_value_pipeline\", missing_value_pipeline),\n",
    "    (\"minimal_pipeline\", minimal_pipeline)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danach holen wir uns den transfromierten Datensatz aus der Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit_transform(train_data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_features = pipeline.named_steps['minimal_pipeline'].named_steps['imputer'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir entfernen das dteday Feature aus der temp_features liste, weil es durch den numeric_values Transformer aus den transformierten Daten entfernt wurde und wandeln das result in einen Dataframe um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(result, columns=result_features, index=train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(result, train_data[\"cnt\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_01 = Pipeline([(\"baseline_model\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_01.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model_01.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = performance_measures(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand der Performace Measures ist zu erkennen, dass sich durch diesen Schritt keine wesentlichen Verbesserungen ergeben haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Data Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der deskriptiven Statistik ist ersichltich, dass das Minimum der Variable `windspeed` bei `-1` liegt. \n",
    "\n",
    "Da die Windgeschwindigkeit nicht negativ sein kann, treffen wir die Annahme, dass es sich hierbei um Fehler handeln muss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier würde sich ggf. eine Regression Imputation zum Auffüllen der vier fehlenden Werte anbieten.\n",
    "\n",
    "Aus der Correlation Matrix sehen wir aber, dass `windspeed` kaum mit anderen Features korreliert. Aus diesem Grund entfernen wir die Werte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierfür nutzen wir folgende Funktion, welche die negativen Werte einer Spalte dropt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df: pd.DataFrame, target: str, thresh: Union[int, float]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop rows based on a threshold.\n",
    "    \n",
    "    Args:\n",
    "        df: pd.DataFrame: The DataFrame to be transformed.\n",
    "        target: str: The target column.\n",
    "        threshold: float: The threshold for dropping rows.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.loc[df[target] >= thresh]\n",
    "        print(f\"{df.shape[0]} rows remaining\")\n",
    "    except KeyError:\n",
    "        print(f\"{target} not found in DataFrame\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weekday(df: pd.DataFrame, target: str, date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Maps weekday based on date column values.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame\n",
    "        date (str): date column name (in datetime64 format)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'weekday' column mapped based on date column values.\n",
    "    \"\"\"\n",
    "    if date in df.columns:\n",
    "        df[target] = (df[date].dt.dayofweek + 1) % 7\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie im letzten Schritt definieren wir den `CustomTransformer`, um die Werte von `weekday` zu korrigieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows_transformer = CustomTransformer(function=drop_rows, config={\"target\": \"windspeed\", \"thresh\": 0.0})\n",
    "\n",
    "weekday_by_date = CustomTransformer(function=process_weekday, config={\"target\": \"weekday\", \"date\": \"dteday\"})\n",
    "\n",
    "data_correction_pipeline = Pipeline([\n",
    "    (\"drop_rows\", drop_rows_transformer),\n",
    "    (\"weekday_by_date\", weekday_by_date)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"missing_value_pipeline\", missing_value_pipeline),\n",
    "    (\"data_correction_pipeline\", data_correction_pipeline),\n",
    "    (\"minimal_pipeline\", minimal_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit_transform(train_data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_features = pipeline.named_steps['minimal_pipeline'].named_steps['imputer'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_indices = train_data[train_data['windspeed'] > 0.0].index\n",
    "result = pd.DataFrame(result, columns=result_features, index=filtered_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(result, train_data.loc[filtered_indices][\"cnt\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_02 = Pipeline([(\"baseline_model\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_02.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model_02.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = performance_measures(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Daten korregiert wurden, ist zu erkennen, dass einige Ausreißer mit den falschen Werten von `windspeed` entfernt wurden.\n",
    "\n",
    "Zudem ist eine leichte Verbesserung in den Preformance Measures zu erkennen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist zu sehen, dass die Gesamtanzahl der geliehenen Fahrräder und die Anzahl der durch `casual` Nutzer geliehenen Fahrräder unrealistisch hohe Maximalwerte aufweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['casual', 'cnt']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscores = stats.zscore(train_data[\"casual\"])\n",
    "thresh = 3.0\n",
    "outliers = train_data[abs(zscores) > thresh]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da `cnt` die Summe aus `casual` und `registered` ist, werden beim Entfernen der Outlier von `casual` auch die Outlier von `cnt` entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df: pd.DataFrame, target: str, threshold: float = 3.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop outliers from target column based on Z-scores.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to process.\n",
    "        target (str): Target column name for outlier removal.\n",
    "        threshold (float): Z-score threshold to consider a data point an outlier.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with outliers removed from the specified target column.\n",
    "    \"\"\"\n",
    "    if target in df.columns:\n",
    "        # Calculate Z-scores for the target column\n",
    "        z_scores = stats.zscore(df[target].dropna())\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers = df[abs(z_scores) > threshold]\n",
    "        \n",
    "        df = df.drop(outliers.index, errors=\"ignore\")\n",
    "\n",
    "        print(f\"{df.shape[0]} rows remaining after removing outliers.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outliers_transformer = CustomTransformer(function=remove_outliers, config={\"target\": \"casual\"})\n",
    "\n",
    "drop_feature_01 = CustomTransformer(function=drop_features, config={\"columns\": [\"cnt\", \"casual\", \"registered\"]})\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"missing_value_pipeline\", missing_value_pipeline),\n",
    "    (\"data_correction_pipeline\", data_correction_pipeline),\n",
    "    (\"remove_outliers\", remove_outliers_transformer),\n",
    "    (\"drop_feature_01\", drop_feature_01),\n",
    "    (\"minimal_pipeline\", minimal_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_features = pipeline.named_steps['minimal_pipeline'].named_steps['imputer'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = abs(stats.zscore(train_data[\"casual\"].dropna()))\n",
    "filtered_indices = train_data[(z_scores < 3) & (train_data['windspeed'] > 0.0)].index\n",
    "result = pd.DataFrame(result, columns=result_features, index=filtered_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(result, train_data.loc[filtered_indices][\"cnt\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_03 = Pipeline([(\"baseline_model\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_03.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model_03.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = performance_measures(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist zu erkennen, dass die Performace Measures, nach der Entfernung der Outliers, eine wesentliche Verbesserung zeigen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Variablen `weekday`, `season` und `weathersit` erstellen wir mittels One Hot Encoding separate binäre Spalten für jede Ausprägung. \n",
    "\n",
    "Anschließend entfernen wir die ursprünglichen drei Spalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(\n",
    "    [('one_hot_encoder', OneHotEncoder(sparse_output=False, drop=\"first\", dtype=int), ['weekday', 'season', 'weathersit'])],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_01 = Pipeline([\n",
    "    (\"missing_value_pipeline\", missing_value_pipeline),\n",
    "    (\"data_correction_pipeline\", data_correction_pipeline),\n",
    "    (\"remove_outliers\", remove_outliers_transformer),\n",
    "    (\"drop_feature_01\", drop_feature_01),\n",
    "    (\"numeric_values\", numeric_values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_02 = Pipeline([\n",
    "    (\"column_transformer\", column_transformer),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"pipeline_01\", pipeline_01),\n",
    "    (\"pipeline_02\", pipeline_02)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_features = pipeline.named_steps[\"pipeline_02\"].named_steps['column_transformer'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = abs(stats.zscore(train_data[\"casual\"].dropna()))\n",
    "filtered_indices = train_data[(z_scores < 3) & (train_data['windspeed'] > 0.0)].index\n",
    "result = pd.DataFrame(result, columns=result_features, index=filtered_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(result, train_data.loc[filtered_indices][\"cnt\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_04 = Pipeline([(\"baseline_model\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_04.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model_04.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = performance_measures(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem Anwenden von One Hot Encoding zeigt sich erneut eine leicht Verbesserung der Performance Measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Normalisierung "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden alle Features mit Hilfe eines ```StandardScalers``` normalisiert, um den Einfluss der Gewichtung einzelner Features, aufgrund ihrer Wertebereiche, zu eliminiern.\n",
    "\n",
    "Es ist zu erwarten, dass der Mean Absolute Error durch die Normalisierung geringer wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer_02 = ColumnTransformer(\n",
    "    [('one_hot_encoder', OneHotEncoder(sparse_output=False, drop=\"first\", dtype=int), ['weekday', 'season', 'weathersit'])],\n",
    "    remainder=StandardScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_02 = Pipeline([\n",
    "    (\"column_transformer_02\", column_transformer_02),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"pipeline_01\", pipeline_01),\n",
    "    (\"pipeline_02\", pipeline_02)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = abs(stats.zscore(train_data[\"casual\"].dropna()))\n",
    "filtered_indices = train_data[(z_scores < 3) & (train_data['windspeed'] > 0.0)].index\n",
    "result = pd.DataFrame(result, columns=result_features, index=filtered_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(result, train_data.loc[filtered_indices][\"cnt\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_05 = Pipeline([(\"baseline_model\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_05.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model_05.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = performance_measures(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Anwenden des `StandardScaler` zeigt sich eine leichte Verbesserung des Mean Absolute Errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Multicolinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Korrelationsmatrix ist zu sehen, dass die Variablen `atemp`, und `temp` eine starke Korrelation aufweisen. \n",
    "\n",
    "Dies kann später zu Problemen bei der Regression führen, daher werden wir nur `atemp` behalten.\n",
    "\n",
    "Wir verwenden die gefühlte Temperatur, da diese zu einem Grad aus der tatasächlichen Temperatur und der Luftfeuchte hervorgeht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_feature_02 = CustomTransformer(function=drop_features, config={\"columns\": [\"temp\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_01 = Pipeline([\n",
    "    (\"missing_value_pipeline\", missing_value_pipeline),\n",
    "    (\"data_correction_pipeline\", data_correction_pipeline),\n",
    "    (\"remove_outliers\", remove_outliers_transformer),\n",
    "    (\"drop_feature\", drop_feature),\n",
    "    (\"drop_feature_01\", drop_feature_01),\n",
    "    (\"drop_feature_02\", drop_feature_02),\n",
    "    (\"numeric_values\", numeric_values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"pipeline_01\", pipeline_01),\n",
    "    (\"pipeline_02\", pipeline_02)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_features = pipeline.named_steps[\"pipeline_02\"].named_steps['column_transformer_02'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = abs(stats.zscore(train_data[\"casual\"].dropna()))\n",
    "filtered_indices = train_data[(z_scores < 3) & (train_data['windspeed'] > 0.0)].index\n",
    "result = pd.DataFrame(result, columns=result_features, index=filtered_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(result, train_data.loc[filtered_indices][\"cnt\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_06 = Pipeline([(\"baseline_model\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_06.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model_06.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = performance_measures(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Entfernen von `temp` sehen wir eine leichte Verschlechterung des Mean Absolute Errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basierend auf der am Ende von Task 1 aufgestellen Vermutung, dass `hum` mit der gefühlten Temperatur `atemp` Colienar ist, schauen wir uns im Folgenden die Performance des Modell nochmal ohne `hum` an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_feature_03 = CustomTransformer(function=drop_features, config={\"columns\": [\"hum\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_01 = Pipeline([\n",
    "    (\"missing_value_pipeline\", missing_value_pipeline),\n",
    "    (\"data_correction_pipeline\", data_correction_pipeline),\n",
    "    (\"remove_outliers\", remove_outliers_transformer),\n",
    "    (\"drop_feature\", drop_feature),\n",
    "    (\"drop_feature_01\", drop_feature_01),\n",
    "    (\"drop_feature_02\", drop_feature_02),\n",
    "    (\"drop_feature_03\", drop_feature_03),\n",
    "    (\"numeric_values\", numeric_values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"pipeline_01\", pipeline_01),\n",
    "    (\"pipeline_02\", pipeline_02)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_features = pipeline.named_steps[\"pipeline_02\"].named_steps['column_transformer_02'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = abs(stats.zscore(train_data[\"casual\"].dropna()))\n",
    "filtered_indices = train_data[(z_scores < 3) & (train_data['windspeed'] > 0.0)].index\n",
    "result = pd.DataFrame(result, columns=result_features, index=filtered_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(result, train_data.loc[filtered_indices][\"cnt\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_07 = Pipeline([(\"baseline_model\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = baseline_model_07.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model_07.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = performance_measures(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Enfernen von `hum` führt zu einer leichten Verbesserung der Performance Measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuletzt exportieren wir das Datenset als CSV und speichern das Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pipeline.fit_transform(test_data)\n",
    "test_result_features = pipeline.named_steps[\"pipeline_02\"].named_steps['column_transformer_02'].get_feature_names_out()\n",
    "z_scores = abs(stats.zscore(test_data[\"casual\"].dropna()))\n",
    "test_filtered_indices = test_data[(z_scores < 3) & (test_data['windspeed'] > 0.0)].index\n",
    "test_result = pd.DataFrame(test_result, columns=test_result_features, index=test_filtered_indices)\n",
    "test_result[\"cnt\"] = test_data.loc[test_filtered_indices][\"cnt\"]\n",
    "test = test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"cnt\"] = train_data.loc[filtered_indices][\"cnt\"]\n",
    "train = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = baseline_model_07[\"baseline_model\"]\n",
    "train.to_csv(f\"../data/BikeRentalDaily_train_processed.csv\", sep=\";\")\n",
    "test.to_csv(f\"../data/BikeRentalDaily_test_processed.csv\", sep=\";\")\n",
    "\n",
    "model_filename = '../models/final_model.pkl'\n",
    "pickle.dump(final_model, open(model_filename, 'wb'))\n",
    "model_features = '../models/final_model_features.pkl'\n",
    "pickle.dump(result.columns, open(model_features, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validierung des Modell mit den Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(f\"../data/BikeRentalDaily_test_processed.csv\", sep=\";\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(model_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pickle.load(open(model_features, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = validation_data.columns.drop([\"cnt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation_data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = performance_measures(validation_data[\"cnt\"], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(validation_data[\"cnt\"], y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introduction-data-science",
   "language": "python",
   "name": "introduction-data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
