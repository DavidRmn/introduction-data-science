{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction-Data-Science Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lena Breitberg, Doreen Mack, David Riethmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union\n",
    "from scipy.stats import zscore\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden zunächst unsere beiden Datensätze in ```DataFrames```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('/home/davidrmn/Studies/introduction-data-science/data/BikeRentalDaily_test.csv', sep=';', index_col='instant')\n",
    "train_data = pd.read_csv('/home/davidrmn/Studies/introduction-data-science/data/BikeRentalDaily_train.csv', sep=';', index_col='instant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun definieren wir eine Funktion, um später alle Modelle innerhalb dieses Notebooks miteinander visuell vergleichen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparative_bar_charts(df, metrics):\n",
    "    num_metrics = len(metrics)\n",
    "    fig, axs = plt.subplots(1, num_metrics, figsize=(5 * num_metrics, 6))\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        df[metric].plot(kind='bar', ax=axs[i])\n",
    "        axs[i].set_title(metric)\n",
    "        axs[i].set_ylabel(metric)\n",
    "        axs[i].set_xlabel('Models')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Metrics to compare\n",
    "metrics = [\"R2\", \"Adjusted_R2\", \"MAE\", \"RMSE\"]\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desweiteren definieren wir Funktionen zur Visualisierung der Performance unserer Modelle.\n",
    "Als Kriterien für die Performancemessung nutzen wir `R²`, `Adjusted R²`, `MAE` sowie `RMSE`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `R²`-Wert (Bestimmtheitsmaß) liegt zwischen 0 und 1, wobei 1 bedeutet, dass das Modell die gesamte Variation der abhängigen Variablen erklärt, und 0 bedeutet, dass das Modell keine über die Vorhersage des Durchschnitts hinausgehende Erklärungskraft hat. Ein R²-Wert von 0,5 zeigt beispielsweise, dass 50% der Schwankungen der tatsächlichen Werte durch das Modell erklärt werden können. Dies bedeutet, dass das Modell einen mäßigen Erfolg bei der Erklärung der beobachteten Schwankungen hat. Es gibt jedoch noch Raum für Verbesserungen, da 50 % der Schwankungen nicht durch das Modell erklärt werden.\n",
    "\n",
    "Der `Adjusted R²`-Wert ist eine erweiterte Version des `R²`-Werts und berückstigt zusätzliche Informationen über die Anzahl der verwendeten unabhängigen Variable. Die Verwendung von `Adjusted R²` bietet eine bereinigte Bewertung der Modellanpassung und hilft Overfitting besser zu beurteilen.\n",
    "\n",
    "Der `MAE` steht für \"Mean Absolute Error\" (Durchschnittlicher Absoluter Fehler) und wird verwendet, um die durchschnittliche absolute Abweichung zwischen den tatsächlichen und vorhergesagten Werten zu quantifizieren. Er wird bevorzugt, wenn Ausreißer in den Daten weniger stark gewichtet werden sollen, da der absolute Betrag genommen wird. Je niedriger der MAE, desto besser ist die Modellleistung. Wenn beispielsweise der MAE 0 ist, bedeutet dies, dass das Modell perfekte Vorhersagen getroffen hat. Der MAE ist leicht zu interpretieren, da er angibt, um wie viel Einheiten die durchschnittliche Vorhersage des Modells von den tatsächlichen Werten abweicht.\n",
    "\n",
    "Der `RMSE` steht für Root Mean Squared Error und gibt die durchschnittliche quadratische Abweichung zwischen den tatsächlichen und vorhergesagten Werten an. Ein niedriger RMSE weist auf genauere Vorhersagen hin. Im Gegensatz zum `MAE` betont der `RMSE` größere Fehler stärker, da er den quadratischen Fehler verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measures(y_true, y_pred, n_predictors) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate R2, Adjusted R2, and MAE\n",
    "    \n",
    "    Args:\n",
    "        y_true: array-like, True values\n",
    "        y_pred: array-like, Predicted values\n",
    "        n_predictors: int, number of predictors used in the model excluding the intercept\n",
    "        \n",
    "    Returns:\n",
    "        r2: float, R2 score\n",
    "        adjusted_r2: float, Adjusted R2 score\n",
    "        mae: float, Mean Absolute Error\n",
    "    \"\"\"\n",
    "    n = len(y_true)  # Number of observations\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - n_predictors - 1))\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    \n",
    "    measures = {\n",
    "        \"R2\": r2,\n",
    "        \"Adjusted_R2\": adjusted_r2,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse\n",
    "    }\n",
    "\n",
    "    print(f\"R2: {r2:.2f}\")\n",
    "    print(f\"Adjusted R2: {adjusted_r2:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    \n",
    "    return measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_vs_predicted(true_v, pred_v) -> None:\n",
    "    \"\"\"\n",
    "    Plot actual vs. predicted values\n",
    "\n",
    "    Args:\n",
    "        true_v: array-like\n",
    "        True values\n",
    "        pred_v: array-like\n",
    "        Predicted values\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = {\"Actual\": true_v, \"Predicted\": pred_v}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    sns.lineplot(data=df, markers=False)\n",
    "\n",
    "    plt.title(\"Actual vs. Predicted Values\")\n",
    "    plt.xlabel(\"Data Points\")\n",
    "    plt.ylabel(\"Values\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir die entsprechende Jahreszeit anhand des Datums bestimmen können, müssen wir die Werte der Spalte `dteday` zuvor in `datetime` Objekte umwandeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatetimeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_column: str, date_format=None):\n",
    "        \"\"\"\n",
    "        Initialize the transformer with target columns and optional datetime format.\n",
    "        \n",
    "        Args:\n",
    "            columns (list): List of column names to convert to datetime.\n",
    "            date_format (str, optional): The datetime format to use for conversion. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.target_column = target_column\n",
    "        self.date_format = date_format\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method. This transformer does not need to learn anything from the data,\n",
    "        so it just returns itself.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame.\n",
    "            y (None, optional): Ignored. Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            self: The fitted transformer.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply the datetime conversion to the target columns.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame to transform.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with the target columns converted to datetime.\n",
    "        \"\"\"\n",
    "        # Ensure we don't modify the original DataFrame\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Convert each target column to datetime\n",
    "        X[self.target_column] = pd.to_datetime(X[self.target_column], format=self.date_format)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_converter = DatetimeConverter(target_column='dteday', date_format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datetime_converter.transform(test_data)\n",
    "train_data = datetime_converter.transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir splitten die ```train_data``` nochmals in `train` und `validate`, sodass wir unsere Modelle im weiteren Verlauf unabhängig von den Testdaten valdieren können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denkbar split auf Zeitlicher Basis -> weil 2 Jahre\n",
    "- e.g. 18 Monate Training 6 Monate Test\n",
    "\n",
    "- durch 'yr' ist Jahrinformation encoded -> random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(train_data, train_data['cnt'], random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outlier from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "Outlier sind ein Bestandteil realer Daten. \n",
    "- Linear Regression eignet sich nicht bei Outlier\n",
    "- Wenn Outlier realistisch sind muss das Modell entsprechend ausgesucht werden, dass es robust dagegen ist.\n",
    "- Domain Wissen notwendig.\n",
    "- In unserem Fall unrealistische Anzahl von cnt.\n",
    "- Wir wollen dennoch die Evaluierung nutzen um die zu zeigen, am ende wird das Modell ohne Outlier evaluiert um die Auswirkung zu zeigen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei Ausreißern im Label eignet sich lineare Regression nur bedingt. Sind die Werte dennoch realistisch, sollte das Modell entsprechend ausgewählt werden, sodass es möglichst robust im Hinblick auf die Ausreißer ist. \n",
    "Hierfür ist zwingend Domänenwissen notwendig. In unserem Fall handelt es sich definitiv um unrealistische Werte für die Variable cnt. Bereits in Task 1 konnten wir die Ausreißer identifizieren.\n",
    "Da wir in den Test- und Validierungsdaten keine Ausreißer entfernen dürfen, werden wir sie für die weiteren Schritte auch nicht aus den Trainingsdaten entfernen.\n",
    "Erst im letzten Schritt, der finalen Evaluierung mit Hilfe der Testdaten, werden wir das Modell einmal komplett ohne Ausreißer evaluieren, um die Auswirkungen aufzuzeigen.\n",
    "\n",
    "Wir entfernen nun für die Trainings- und Testdaten die Ausreißer und speichern sie in den Variablen test_data_no und train_data_no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target: str, threshold: float = 3.0):\n",
    "        \"\"\"\n",
    "        Initialize the transformer with the target column for outlier removal and the Z-score threshold.\n",
    "\n",
    "        Args:\n",
    "            target (str): The target column for outlier removal.\n",
    "            threshold (float): Z-score threshold to consider a data point an outlier.\n",
    "        \"\"\"\n",
    "        self.target = target\n",
    "        self.threshold = threshold\n",
    "        self.outliers = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method for the transformer. Determines which data points in the target column are outliers\n",
    "        based on the Z-score threshold.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame with the target column.\n",
    "            y (None, optional): Not used, for compatibility with scikit-learn's transformer requirements.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted transformer, with outliers identified.\n",
    "        \"\"\"\n",
    "        if self.target in X.columns:\n",
    "            # Calculate Z-scores for the target column\n",
    "            z_scores = zscore(X[self.target].dropna())\n",
    "            # Identify outliers\n",
    "            self.outliers = abs(z_scores) > self.threshold\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transform method for the transformer. Removes outliers from the DataFrame based on the logic\n",
    "        determined in the fit method.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame to transform by removing outliers.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with outliers removed from the specified target column.\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the DataFrame to avoid altering original data\n",
    "        if self.outliers is not None:\n",
    "            # Remove the outliers identified in the fit method\n",
    "            X = X[~X[self.target].index.isin(X[self.target].dropna().index[self.outliers])]\n",
    "            print(f\"{X.shape[0]} rows remaining after removing outliers.\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_remover = OutlierRemover(target='cnt', threshold=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_no = outlier_remover.fit_transform(test_data)\n",
    "train_data_no = outlier_remover.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Minimal Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um unser Baseline Modell trainieren zu können, müssen wir Vorverarbeitungsschritte durchführen. Diese werden in der ```minimal_preprocessing``` Pipeline definiert.\n",
    "\n",
    "Die Summe der beiden Spalten ```casual``` und ```registered``` ergibt den Wert von ```cnt```, diese sind in der Beschreibung des Datensatzes zudem als Labels beschrieben, daher werden sie entfernt.\n",
    "\n",
    "Zudem entfernen wir `dteday`.\n",
    "\n",
    "Alle ```NaN``` Werte werden unter Anwendung des ```SimpleImputer``` durch den jeweiligen Mittlewert aufgefüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_preprocessing = make_column_transformer(\n",
    "    ('drop', ['dteday', 'casual', 'registered', 'cnt']),\n",
    "    (SimpleImputer(strategy='mean'), ['season', 'hum']),\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die einzelnen Schritte der erzeugten Pipeline werden nacheinander ausgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    minimal_preprocessing,\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell wird nun auf dem verarbeiteten Datensatz trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Anschluss nutzten wir die Validierungsdaten, um das Modell damit zu evaluieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = performance_measures(y_validate, y_pred, len(pipeline.named_steps.linearregression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Minimal_Preprocessing'] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus dem Diagramm wird ersichtlich, dass unser ```baseline_model``` aufgrund der Ausreißer noch Schwierigkeiten bei der Vorhersage hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Missing Value Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir bereits in Task 1 erarbeitet haben, weisen die Spalten `season` und `hum` fehlende Werte auf. \n",
    "\n",
    "Im `minimal_preprocessing` werden alle `NaN` Werte mit durch den `SimpleImputer` durch die Methode `mean` aufgefüllt.\n",
    "\n",
    "Nun sollen die fehlenden Werte in der Spalte `season` anhand des vorliegenden Datums in der entsprechenden Zeile aufgefüllt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klasse `SeasonImputer` bestimmt die Jahreszeit anhand eines Datums. \n",
    "\n",
    "Dabei liest sie den Tag des entsprechenden Datums aus und weist diesen der entsprechenden Jahreszeit zu.\n",
    "\n",
    "Es gibt vier fest definierte Zeitfenster, die verwendet werden um den Tag des Jahres in vier Jahreszeiten einzuteilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeasonImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_column: str, date_column: str):\n",
    "        \"\"\"\n",
    "        Initialize the transformer with the name of the target column for the season\n",
    "        and the date column from which the season will be determined.\n",
    "        \n",
    "        Args:\n",
    "            target_column (str): The name of the column to store the imputed season.\n",
    "            date_column (str): The name of the date column to determine the season from.\n",
    "        \"\"\"\n",
    "        self.target_column = target_column\n",
    "        self.date_column = date_column\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method for the transformer. Since this transformer does not need to learn\n",
    "        anything from the data, it just returns itself.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame.\n",
    "            y (None, optional): Not used, for compatibility with scikit-learn's transformer requirements.\n",
    "        \n",
    "        Returns:\n",
    "            self: The fitted transformer.\n",
    "        \"\"\"\n",
    "        return self  # No fitting necessary\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply the season imputation to the DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame to transform.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with the season imputed.\n",
    "        \"\"\"\n",
    "        # Ensure we don't modify the original DataFrame\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Define the function to determine the season based on the day of the year\n",
    "        def get_season(date):\n",
    "            day_of_year = date.timetuple().tm_yday\n",
    "            if 80 <= day_of_year < 172:\n",
    "                return 2  # Spring\n",
    "            elif 172 <= day_of_year < 265:\n",
    "                return 3  # Summer\n",
    "            elif 265 <= day_of_year < 355:\n",
    "                return 4  # Fall\n",
    "            else:\n",
    "                return 1  # Winter\n",
    "        \n",
    "        # Check if the date column exists and apply the season calculation\n",
    "        if self.date_column in X.columns:\n",
    "            X[self.target_column] = X[self.date_column].apply(lambda x: get_season(x) if pd.notnull(x) else x)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die beiden Teilschritte werden im Folgenden der bestehenden Pipeline hinzugefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_imputer = SeasonImputer(target_column='season', date_column='dteday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    season_imputer,\n",
    "    minimal_preprocessing,\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = performance_measures(y_validate, y_pred, len(pipeline.named_steps.linearregression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Missing_Values'] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anhand der Performace Measures ist zu erkennen, dass sich durch diesen Schritt keine wesentlichen Verbesserungen ergeben haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nachfolgenden Abschnitt wollen wir einige Bereinigungen und Korrekturen vornehmen.\n",
    "\n",
    "Aus der deskriptiven Statistik ist ersichltich, dass das Minimum der Variable `windspeed` bei `-1` liegt. \n",
    "\n",
    "Da die Windgeschwindigkeit nicht negativ sein kann, treffen wir die Annahme, dass es sich hierbei um Fehler handeln muss.\n",
    "\n",
    "Auch bei der Varialbe `weekday` fallen Werte von `-1` auf, die wir korrigieren wollen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst definieren wir einen Custom Transformer, der die Wochentage basierend auf der Datumsspalte, mappt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeekdayMapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_column: str, date_column: str):\n",
    "        \"\"\"\n",
    "        Initialize the transformer with the name of the target column for the weekday\n",
    "        and the date column from which the weekday will be determined.\n",
    "        \n",
    "        Args:\n",
    "            target_column (str): The name of the column to store the mapped weekday.\n",
    "            date_column (str): The name of the date column to determine the weekday from.\n",
    "        \"\"\"\n",
    "        self.target_column = target_column\n",
    "        self.date_column = date_column\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method for the transformer. Since this transformer does not need to learn\n",
    "        anything from the data, it just returns itself.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame.\n",
    "            y (None, optional): Not used, for compatibility with scikit-learn's transformer requirements.\n",
    "        \n",
    "        Returns:\n",
    "            self: The fitted transformer.\n",
    "        \"\"\"\n",
    "        return self  # No fitting necessary\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply the weekday mapping to the DataFrame based on the date column.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame to transform.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with the weekday column mapped.\n",
    "        \"\"\"\n",
    "        # Ensure we don't modify the original DataFrame\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Check if the date column exists and map the weekday\n",
    "        if self.date_column in X.columns:\n",
    "            # Maps the weekday based on the date column, where Monday=0, Sunday=6, then adjusts to Monday=1, Sunday=0\n",
    "            X[self.target_column] = (X[self.date_column].dt.dayofweek + 1) % 7\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgenden Custom Transformer können die fehlerhaften Werte entweder entfernen oder duch den Mittelwert ersetzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_column: str, threshold: Union[int, float] = 0.0, impute_above: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the transformer with the target column for imputation, the threshold value, and the direction of imputation.\n",
    "        \n",
    "        Args:\n",
    "            target_column (str): The target column for imputation.\n",
    "            threshold (Union[int, float]): The threshold value for imputation.\n",
    "            impute_above (bool): If True, values above the threshold will be imputed; if False, values below the threshold will be imputed.\n",
    "        \"\"\"\n",
    "        self.target_column = target_column\n",
    "        self.threshold = threshold\n",
    "        self.impute_above = impute_above\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method for the transformer. Calculates the mean of the target column values either above or below the threshold based on the direction of imputation.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame.\n",
    "            y (None, optional): Not used, for compatibility with scikit-learn's transformer requirements.\n",
    "        \n",
    "        Returns:\n",
    "            self: The fitted transformer.\n",
    "        \"\"\"\n",
    "        if self.target_column in X.columns:\n",
    "            if self.impute_above:\n",
    "                # Calculate mean of values below the threshold for imputing values above it\n",
    "                self.mean_value_ = X.loc[X[self.target_column] <= self.threshold, self.target_column].mean()\n",
    "            else:\n",
    "                # Calculate mean of values above the threshold for imputing values below it\n",
    "                self.mean_value_ = X.loc[X[self.target_column] >= self.threshold, self.target_column].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply the imputation to the target column based on the threshold value and direction of imputation.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The input DataFrame to transform.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with values in the target column imputed based on the specified criteria.\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the DataFrame to avoid altering original data\n",
    "        if self.target_column in X.columns:\n",
    "            if self.impute_above:\n",
    "                # Impute values above the threshold\n",
    "                X.loc[X[self.target_column] > self.threshold, self.target_column] = self.mean_value_\n",
    "            else:\n",
    "                # Impute values below the threshold\n",
    "                X.loc[X[self.target_column] < self.threshold, self.target_column] = self.mean_value_\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die beiden Teilschritte werden im Folgenden der bestehenden Pipeline hinzugefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_weekday = WeekdayMapper(target_column='weekday', date_column='dteday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_mean_imputer = ThresholdImputer(target_column='windspeed', threshold=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    season_imputer,\n",
    "    correct_weekday,\n",
    "    windspeed_mean_imputer,\n",
    "    minimal_preprocessing,\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = performance_measures(y_validate, y_pred, len(pipeline.named_steps.linearregression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Data_Correction'] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Daten korrigiert wurden, ist keine nennenswerte Verbesserung der Preformance Measures zu erkennen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt, Feature Transformation, möchten wir zunächst `One Hot Encoding` auf ausgewählte Varialen anwenden. Im Anschluss normalisieren wir ausgewählte Variablen mit Hilfe eines `StandardScalers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir möchten für die Variablen `weekday`, `season` und `weathersit` mittels One Hot Encoding separate binäre Spalten für jede Ausprägung erstellen. \n",
    "\n",
    "Da wir für die Variable `weekday` sieben neue Variablen generieren würden und die einzelnen Wochentage nicht aussagekräftig sind, möchten wir die Tage in Wochentage und Wochenende gruppieren. Durch diese Unterscheidung bietet sich eine höhere Aussagekraft. Im gleichen Zuge verliert die Variable `workingday` an Aussagekraft, weshalb wir diese im weiteren Verlauf entfernen.\n",
    "\n",
    "Auch für die Variable `month` könnte One Hot Encoding durchgeführt werden und es würden zwölf neue Variablen generiert werden. Die einzig sinnvolle Möglichkeit zur Gruppierung sind nach unserer Meinung die Jahreszeiten. Im Datensatz liegt bereits die Variable `season` vor. Aufgrund dessen verzichten wir auf One Hot Encoding für `month` und entfernen die Variable stattdessen. Die Variable `season` hingegen soll weiterhin bestehen bleiben.\n",
    "\n",
    "Desweiteren werden die ursprünglichen drei Spalten `weekday`, `season` und `weathersit` entfernt.\n",
    "\n",
    "Wir definieren zur Durchführung dieser Teilschritte folgenden Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupOneHotEncodedTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, grouping_info):\n",
    "        \"\"\"\n",
    "        Initialize the transformer with the grouping information for one-hot encoded columns.\n",
    "        \n",
    "        Parameters:\n",
    "        - grouping_info: Dictionary where keys are the new column names for the groups\n",
    "                         and values are lists of columns to be grouped.\n",
    "        \"\"\"\n",
    "        self.grouping_info = grouping_info\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method for the transformer. This transformer does not need to learn anything from the data,\n",
    "        so it just returns itself.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: pandas DataFrame containing one-hot encoded columns.\n",
    "        - y: Not used, for compatibility with scikit-learn's transformer requirements.\n",
    "        \n",
    "        Returns:\n",
    "        - self: The fitted transformer.\n",
    "        \"\"\"\n",
    "        return self  # No fitting necessary\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply the grouping to the one-hot encoded columns based on the provided grouping information.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: pandas DataFrame to transform.\n",
    "        \n",
    "        Returns:\n",
    "        - DataFrame with grouped columns.\n",
    "        \"\"\"\n",
    "        # Ensure we don't modify the original DataFrame\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        for new_col, columns_to_group in self.grouping_info.items():\n",
    "            # Create a new column for the group, using `max` as an example aggregation\n",
    "            X_transformed[new_col] = X_transformed[columns_to_group].max(axis=1)\n",
    "            \n",
    "            # Drop the original columns that were grouped\n",
    "            X_transformed.drop(columns=columns_to_group, inplace=True)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Cleans DataFrame column names by removing 'remainder__' prefixes.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame with column names to clean.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with cleaned column names.\n",
    "    \"\"\"\n",
    "    # Use a dictionary comprehension to create a mapping of old to new names\n",
    "    rename_map = {col: '__'.join(col.split('__')[-1:]) for col in df.columns}\n",
    "    \n",
    "    # Rename the columns using the mapping\n",
    "    df_renamed = df.rename(columns=rename_map)\n",
    "    \n",
    "    return df_renamed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die neuen Teilschritte werden der bestehenden Pipeline hinzugefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_transformer = FunctionTransformer(clean_column_names)\n",
    "weekend_group = GroupOneHotEncodedTransformer(grouping_info={'weekend': ['weekday_0', 'weekday_6']})\n",
    "weekday_group = GroupOneHotEncodedTransformer(grouping_info={'weekday': ['weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(sparse_output=False, dtype=int), ['season', 'weekday']),\n",
    "    (OneHotEncoder(sparse_output=False, drop='first', dtype=int), ['weathersit']),\n",
    "    (SimpleImputer(strategy='mean'), ['hum']),\n",
    "    ('drop', ['workingday', 'mnth', 'dteday', 'cnt', 'casual', 'registered']),\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    season_imputer,\n",
    "    correct_weekday,\n",
    "    windspeed_mean_imputer,\n",
    "    feature_transformer,\n",
    "    column_name_transformer,\n",
    "    weekend_group,\n",
    "    weekday_group,\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = performance_measures(y_validate, y_pred, len(pipeline.named_steps.linearregression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['One_Hot_Encoder'] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach derAnwendeung von One Hot Encoding und der Gruppierung zeigt sich erneut eine leicht Verbesserung der Performance Measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Normalisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden die Features mit Hilfe eines ```StandardScalers``` normalisiert, um den Einfluss der Gewichtung einzelner Features aufgrund ihrer Wertebereiche zu eliminiern.\n",
    "\n",
    "Es ist zu erwarten, dass der Mean Absolute Error durch die Normalisierung geringer wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir führen die Normalisierung für die Variablen `temp`, `atemp`, `windspeed`, `leaflets` und `hum` durch. Der Teilschritt wird im Anschluss der bestehenden Pipeline hinzugefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(sparse_output=False, dtype=int), ['season', 'weekday']),\n",
    "    (OneHotEncoder(sparse_output=False, drop='first', dtype=int), ['weathersit']),\n",
    "    (make_pipeline(SimpleImputer(strategy='mean'), StandardScaler()), ['hum']),\n",
    "    (StandardScaler(), ['temp', 'atemp', 'windspeed', 'leaflets']),\n",
    "    ('drop', ['workingday', 'mnth', 'dteday', 'cnt', 'casual', 'registered']),\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    season_imputer,\n",
    "    correct_weekday,\n",
    "    windspeed_mean_imputer,\n",
    "    feature_transformer,\n",
    "    column_name_transformer,\n",
    "    weekend_group,\n",
    "    weekday_group,\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = performance_measures(y_validate, y_pred, len(pipeline.named_steps.linearregression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Normalization'] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Anwenden des `StandardScaler` zeigt sich eine nur geringfügige Verbesserung des Mean Absolute Errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Multikollinearität"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus Task 1 ist zu sehen, dass die Variablen `atemp`, und `temp` eine starke Korrelation sowie hohe VIF-Wert aufweisen.\n",
    "\n",
    "Da Multikollineariät später zu Problemen bei der Regression führen kann, werden wir eine der beiden Variablen, in diesem Falle `temp` entfernen.\n",
    "\n",
    "Wir verwenden die gefühlte Temperatur `atemp` weiter, da diese zum Einen aus der tatasächlichen Temperatur und zum Anderen aus der Luftfeuchtigkeit `hum` hervorgeht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir entfernen die Variable `temp` und fügen diesen Teilschritt der Pipeline hinzu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(sparse_output=False, dtype=int), ['season', 'weekday']),\n",
    "    (OneHotEncoder(sparse_output=False, drop='first', dtype=int), ['weathersit']),\n",
    "    (make_pipeline(SimpleImputer(strategy='mean'), StandardScaler()), ['hum']),\n",
    "    (StandardScaler(), ['atemp', 'windspeed', 'leaflets']),\n",
    "    ('drop', ['temp', 'workingday', 'mnth', 'dteday', 'cnt', 'casual', 'registered']),\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    season_imputer,\n",
    "    correct_weekday,\n",
    "    windspeed_mean_imputer,\n",
    "    feature_transformer,\n",
    "    column_name_transformer,\n",
    "    weekend_group,\n",
    "    weekday_group,\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = pipeline.named_steps.linearregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = performance_measures(y_validate, y_pred, len(pipeline.named_steps.linearregression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Multicolinearity'] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(y_validate, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Entfernen von `temp` sehen wir eine leichte Verbesserung der Performance Measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im letzten Schritt folgt nun die Evaluierung unseres Modells anhand der Testdaten.\n",
    "\n",
    "Da in den Testdaten noch Ausreißer vorliegen, wird die Evaluierung leicht verzerrt. Um den Einfluss herauszustellen, führen wir die Evaluierung einmal mit den Testdaten durch, die noch Ausreißer enthalten und einmal mit den um die Ausreißer bereinigten Testdaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Testdata with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = performance_measures(test_data['cnt'], y_pred, len(pipeline.named_steps.linearregression.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Performance Measures zeigen eine moderate Performance unseres Modells an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Test_Data_with_Outliers'] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(test_data['cnt'], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der Visualisierung ist ersichltich, dass das Modell die Ausreißer in den Testdaten nicht abbilden kann. Dadurch werden zudem die Performance Measures verzerrt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Testdata without Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutzen wir zur Evaluierung unseres Modell die um Ausreißer bereinigten Testdaten, so zeigen die Performance Measures eine deutlich bessere Performance unseres Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no, X_validate_no, y_train_no, y_validate_no = train_test_split(train_data_no, train_data_no['cnt'], random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    season_imputer,\n",
    "    correct_weekday,\n",
    "    windspeed_mean_imputer,\n",
    "    feature_transformer,\n",
    "    column_name_transformer,\n",
    "    weekend_group,\n",
    "    weekday_group,\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train_no, y_train_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(test_data_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = performance_measures(test_data_no['cnt'], y_pred, len(pipeline.named_steps.linearregression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Test_Data_without_Outliers'] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(test_data_no['cnt'], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der Visualisierung ist nun ersichtlich, dass die Ausreißer in den Testdaten nicht mehr vorliegen und das Modell gut performt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Performance Vergleich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschließend ist aus den folgenden Diagrammen ersichtlich, wie sich die Performance Measures im Laufe des Data Preparation Prozesses verbessert haben. \n",
    "\n",
    "Während die Werte für RMSE und MAE reduziert wurden, haben die Werte für R-Squared sowie Adjusted R-Squared zugenommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_vergleich = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparative_bar_charts(performance_vergleich.T, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportieren der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten werden abschließend exportiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_pipeline = make_pipeline(\n",
    "    season_imputer,\n",
    "    correct_weekday,\n",
    "    windspeed_mean_imputer,\n",
    "    feature_transformer,\n",
    "    column_name_transformer,\n",
    "    weekend_group,\n",
    "    weekday_group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = processing_pipeline.fit_transform(X_train)\n",
    "X_validate = processing_pipeline.fit_transform(X_validate)\n",
    "preprocessed_test_data = processing_pipeline.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = '../models/final_model_task2.pkl'\n",
    "pickle.dump(final_model, open(model_filename, 'wb'))\n",
    "\n",
    "train_data_filename = '../data/train_data_task2.csv'\n",
    "processed_train_data = pd.concat([X_train, y_train], axis=1)\n",
    "processed_train_data.to_csv(train_data_filename, index=False)\n",
    "\n",
    "validation_data_filename = '../data/validation_data_task2.csv'\n",
    "processed_validate_data = pd.concat([X_validate, y_validate], axis=1)\n",
    "processed_validate_data.to_csv(validation_data_filename, index=False)\n",
    "\n",
    "test_data_filename = '../data/test_data_task2.csv'\n",
    "test_data = pd.concat([preprocessed_test_data, test_data['cnt']], axis=1)\n",
    "test_data.to_csv(test_data_filename, index=False)\n",
    "\n",
    "print(\"All files saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introduction-data-science",
   "language": "python",
   "name": "introduction-data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
