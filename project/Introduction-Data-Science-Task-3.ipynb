{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction-Data-Science Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lena Breitberg, Doreen Mack, David Riethmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (0.24.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.23.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "auto-sklearn 0.15.0 requires scikit-learn<0.25.0,>=0.24.0, but you have scikit-learn 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='24.0'),)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_config() got an unexpected keyword argument 'transform_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#from lazypredict.Supervised import LazyRegressor\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mset_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpandas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: set_config() got an unexpected keyword argument 'transform_output'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn._config import set_config\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, PassiveAggressiveRegressor, RANSACRegressor, HuberRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "#from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "import helpers\n",
    "set_config(transform_output=\"pandas\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/BikeRentalDaily_test.csv', sep=';', index_col='instant')\n",
    "train_data = pd.read_csv('../data/BikeRentalDaily_train.csv', sep=';', index_col='instant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_converter = helpers.DatetimeConverter(target_column='dteday', date_format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datetime_converter.transform(test_data)\n",
    "train_data = datetime_converter.transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sind die finalen Schritte des Preprocessing aus Taks 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_imputer = helpers.SeasonImputer(target_column='season', date_column='dteday')\n",
    "correct_weekday = helpers.WeekdayMapper(target_column='weekday', date_column='dteday')\n",
    "windspeed_mean_imputer = helpers.ThresholdImputer(target_column='windspeed', threshold=0.0)\n",
    "column_name_transformer = FunctionTransformer(helpers.clean_column_names)\n",
    "weekend_group = helpers.GroupOneHotEncodedTransformer(grouping_info={'weekend': ['weekday_0', 'weekday_6']})\n",
    "weekday_group = helpers.GroupOneHotEncodedTransformer(grouping_info={'weekday': ['weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(sparse_output=False, dtype=int), ['season', 'weekday']),\n",
    "    (OneHotEncoder(sparse_output=False, drop='first', dtype=int), ['weathersit']),\n",
    "    (make_pipeline(SimpleImputer(strategy='mean'), StandardScaler()), ['hum']),\n",
    "    (StandardScaler(), ['atemp', 'windspeed', 'leaflets']),\n",
    "    ('drop', ['temp', 'workingday', 'mnth', 'dteday','cnt', 'casual', 'registered']),\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = make_pipeline(\n",
    "    season_imputer,\n",
    "    correct_weekday,\n",
    "    windspeed_mean_imputer,\n",
    "    feature_transformer,\n",
    "    column_name_transformer,\n",
    "    weekend_group,\n",
    "    weekday_group\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_remover = helpers.OutlierRemover(target='cnt', threshold=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_no = outlier_remover.fit_transform(test_data)\n",
    "train_data_no = outlier_remover.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(train_data, train_data['cnt'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no, X_validate_no, y_train_no, y_validate_no = train_test_split(train_data_no, train_data_no['cnt'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(reg, X_test, y_test, diabetes):\n",
    "    feature_importance = reg.feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "    plt.yticks(pos, np.array(diabetes.feature_names)[sorted_idx])\n",
    "    plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "    result = permutation_importance(\n",
    "        reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    "    )\n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(\n",
    "        result.importances[sorted_idx].T,\n",
    "        vert=False,\n",
    "        labels=np.array(diabetes.feature_names)[sorted_idx],\n",
    "    )\n",
    "    plt.title(\"Permutation Importance (test set)\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measures(y_true, y_pred, n_predictors) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate R2, Adjusted R2, and MAE\n",
    "    \n",
    "    Args:\n",
    "        y_true: array-like, True values\n",
    "        y_pred: array-like, Predicted values\n",
    "        n_predictors: int, number of predictors used in the model excluding the intercept\n",
    "        \n",
    "    Returns:\n",
    "        r2: float, R2 score\n",
    "        adjusted_r2: float, Adjusted R2 score\n",
    "        mae: float, Mean Absolute Error\n",
    "    \"\"\"\n",
    "    n = len(y_true)  # Number of observations\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - n_predictors - 1))\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    \n",
    "    measures = {\n",
    "        \"R2\": r2,\n",
    "        \"Adjusted_R2\": adjusted_r2,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse\n",
    "    }\n",
    "\n",
    "    print(f\"R2: {r2:.2f}\")\n",
    "    print(f\"Adjusted R2: {adjusted_r2:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    \n",
    "    return measures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualVsPredictChart(true_v, pred_v, model: str = \"\"):\n",
    "    data = {\"Actual\": true_v, \"Predicted\": pred_v}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create a lineplot with Seaborn\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    sns.lineplot(data=df, markers=False)\n",
    "\n",
    "    plt.title(f\"{model} Actual vs. Predicted Values\")\n",
    "    plt.xlabel(\"Data Points\")\n",
    "    plt.ylabel(\"Values\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = make_pipeline(\n",
    "    preprocessing_pipeline,\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = performance_measures(y_validate, y_pred, len(baseline.named_steps.linearregression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualVsPredictChart(y_validate, y_pred, \"Baseline Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Initial Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "    time_left_for_this_task=120\n",
    "    )\n",
    "automl.fit(X_train, y_train, dataset_name='BikeRentalDaily')\n",
    "\n",
    "ensemble_dict = automl.show_models()\n",
    "print(ensemble_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lazy_predictions(models, predictions, y_validate, n_predictors: int):\n",
    "    \"\"\"\n",
    "    Visualize the predictions from the lazy regressor\n",
    "\n",
    "    Args:\n",
    "        predictions: dict, predictions from the lazy regressor\n",
    "    \"\"\"\n",
    "    models = models.sort_values(by='R-Squared', ascending=False)\n",
    "    for model_name in models.iloc[: 3].index:\n",
    "        y_pred = predictions[model_name].to_list()\n",
    "        _ = performance_measures(y_validate, y_pred, n_predictors)\n",
    "        actualVsPredictChart(y_validate, y_pred, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Evaluierung verschiedener Algorithmen nutzen wir die Bibliothek Lazy Predict bzw. aus dieser Bibliothek den Lazy Regressor. Diese wurde von Shankar Rao Pandala entwickelt. Es handelt sich um eine Wrapper-Bibliothek, die auf vielen verschiedenen Machine Learning Bibliotheken und -Algorithmen basiert.\n",
    "Der Lazy Regressor automatisiert den Auswahlprozess geeigneter Regressionsmodelle, indem eine Vielzahl von Modellen auf den vorliegenden Datensatz angewandt wird. Dieser wird zuvor durch minimales Preprocessing für den Auswahlprozess vorbereitet z.B. mittels Scaling und Imputation. \n",
    "Im Anschluss werden die Modelle anhand der gängigen Metriken, in unserem Fall R-Squared und MAE bewertet. Es ist wichtig zu beachten, dass kein automatisiertes Hyperparamter Tuning durchgeführt wird.\n",
    "Dieses Vorgehen bietet vor allem den Vorteil, dass des gesamte Prozess automatisiert abläuft und die Bibliothek sehr einfach anzuwenden ist. Zusätzlich wird eine erhebliche Zeitersparnis ermöglicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_validate = preprocessing_pipeline.fit_transform(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_regressor = LazyRegressor(verbose=0, ignore_warnings=True, predictions=True)\n",
    "models, predictions = lazy_regressor.fit(X_train, X_validate, y_train, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem DataFrame sind die Metriken der jeweiligen Modelltypen zu sehen. Zu erkennen ist, dass der Typ `GradientBoostingRegressor` nach allen Metriken den höchsten Wert für Adjusted R-Squared erzielt. Weitere gut abschneidende Modelltypen sind der `XGBRegressor` und der `ExtraTreesRegressor`. Für das Parametertuning werden alle drei Typen näher betrachtet, da es sein könnte, dass eines der drei Modelle deutlich besser abschneidet, sobald die Hyperparameter angepasst werden. Den Modelltyp `HistGradientBoostingRegressor` werden wir nicht verwenden, da dieser besonders ähnliche Eigenschaften aufweist, wie der `GradientBoostingRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf dieser Grafik ist ein Vergleich zwischen tatsächlichen und vorhergesagten Daten des `GradientBoostingRegressor` zu sehen. Es ist zu erkennen, dass es einige Ausreißer nach unten gibt, die das Modell nicht abbilden konnte. Bei dem Wert am rechten Ende des Graphen konnte das Modell dies jedoch sehr gut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_lazy_predictions(models, predictions, y_validate, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no = preprocessing_pipeline.fit_transform(X_train_no)\n",
    "X_validate_no = preprocessing_pipeline.fit_transform(X_validate_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_regressor_no = LazyRegressor(verbose=0, ignore_warnings=True, predictions=True)\n",
    "models_no, predictions_no = lazy_regressor_no.fit(X_train_no, X_validate_no, y_train_no, y_validate_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_lazy_predictions(models_no, predictions_no, y_validate_no, X_train_no.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Hyperparamter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden probieren wir verschiedene Kombinationen von Hyperparametern für die drei ausgewählten Algorithmen aus, um jeweils die beste Leistung des Modells zu erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANSACRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = LinearRegression()\n",
    "ransac_regressor = RANSACRegressor(estimator=base_estimator, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'rfecv__min_features_to_select': [5, 10, 15],\n",
    "    'ransacregressor__min_samples': [0.5, 0.6, 0.7],  # Proportion of total samples or absolute number\n",
    "    'ransacregressor__max_trials': [50, 100, 150],  # Maximum number of iterations for random sample selection.\n",
    "    'ransacregressor__stop_probability': [0.99, 0.999],  # Confidence level of the algorithm to terminate iterations\n",
    "    'ransacregressor__loss': ['absolute_error', 'squared_error'],  # Loss function to be used\n",
    "}\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    preprocessing_pipeline,\n",
    "    ransac_regressor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train_data, train_data['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_regressor = HuberRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'rfecv__min_features_to_select': [5, 10, 15],\n",
    "    'huberregressor__epsilon': [1.1, 1.2, 1.3],  # Epsilon parameter for the Huber loss\n",
    "    'huberregressor__alpha': [0.0001, 0.001, 0.01],  # Regularization parameter\n",
    "    'huberregressor__max_iter': [100, 200, 300],  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "recursive_feature_elimination = RFECV(estimator=huber_regressor, step=1, cv=5, scoring='r2')\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    preprocessing_pipeline,\n",
    "    recursive_feature_elimination,\n",
    "    huber_regressor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train_data, train_data['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_regressor = GradientBoostingRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'rfecv__min_features_to_select': [5, 10, 15],\n",
    "    'gradientboostingregressor__n_estimators': [100, 200, 300],  # Number of boosting stages\n",
    "    'gradientboostingregressor__learning_rate': [0.01, 0.1, 0.2],  # Learning rate shrinks the contribution of each tree\n",
    "    'gradientboostingregressor__max_depth': [3, 4, 5],  # Maximum depth of the individual estimators\n",
    "    'gradientboostingregressor__min_samples_split': [2, 3, 4],  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "recursive_feature_elimination = RFECV(estimator=gradient_boosting_regressor, step=1, cv=5, scoring='r2')\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    preprocessing_pipeline,\n",
    "    recursive_feature_elimination,\n",
    "    gradient_boosting_regressor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train_data_no, train_data_no['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passive_aggressive_regressor = PassiveAggressiveRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'rfecv__min_features_to_select': [5, 10, 15],\n",
    "    'passiveaggressiveregressor__C': [0.01, 0.1, 1, 10],\n",
    "    'passiveaggressiveregressor__max_iter': [1000, 1500, 2000],\n",
    "    'passiveaggressiveregressor__tol': [1e-3, 1e-4],\n",
    "    'passiveaggressiveregressor__early_stopping': [True, False],\n",
    "}\n",
    "\n",
    "recursive_feature_elimination = RFECV(passive_aggressive_regressor, n_jobs=-1, scoring='r2', cv=5)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    preprocessing_pipeline,\n",
    "    recursive_feature_elimination,\n",
    "    passive_aggressive_regressor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train_data_no, train_data_no['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_regressor = HuberRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'rfecv__min_features_to_select': [5, 10, 15],\n",
    "    'huberregressor__epsilon': [1.1, 1.2, 1.3],  # Epsilon parameter for the Huber loss\n",
    "    'huberregressor__alpha': [0.0001, 0.001, 0.01],  # Regularization parameter\n",
    "    'huberregressor__max_iter': [100, 200, 300],  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "recursive_feature_elimination = RFECV(estimator=huber_regressor, step=1, cv=5, scoring='r2')\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    preprocessing_pipeline,\n",
    "    recursive_feature_elimination,\n",
    "    huber_regressor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train_data_no, train_data_no['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Loss**: Loss bezeichnet das Verlustmaß. Dieses soll minimiert werden. Wir verwenden die Methode \"quantile\" d.h. die Quantilregression. Hierdurch wird der quantile Fehler minimiert, was zu robusteren Regressionen führen kann. Desweiteren stehen die Methoden squared_error (Minimierung des mittleren quadratischer Fehlers zwischen vorhergesagten und tatsächlichen Werten), absolute_error (Minimierung des mittleren absoluten Fehlers zwischen vorhergesagten und tatsächlichen Werten) sowie huber (Kombination aus quadratischem und absolutem Fehler) zur Auswahl.\n",
    "- **Alpha**: Der Alpha-Wert beeinflusst die Gewichtung der Residuals in der Quantilregression. Durch die Variation von alpha kann der Fokus auf unterschiedliche Quantile gelegt werden. Wir verwenden eine Liste von Werten von 0.2 bis 0.8 in Schritten von 0.2.\n",
    "- **Criterion**: Hiermit ist das Kriterium gemeint, das zur Auswahl der besten Aufteilung in jedem Entscheidungsbaumknoten verwendet wird. In diesem Fall werden 'friedman_mse' und 'squared_error' verwendet. 'friedman_mse' ist eine verbesserte Version des mittleren quadratischen Fehlers (MSE) für Gradient Boosting.\n",
    "- **Learning Rate**: Die Lernrate steuert, wie stark jeder Baum die vorherigen Bäume korrigiert. Eine niedrige Lernrate erfordert im Normalfall mehr Bäume, um den gleichen Effekt zu erzielen, kann jedoch im Gegenzug zu einer besseren Generalisierung führen. Hier werden Werte von 0.1 bis 0.5 in Schritten von 0.1 verwendet.\n",
    "- **Max Depth**: Max Depth meint die maximale Tiefe eines einzelnen Entscheidungsbaums. Eine tiefere Baumstruktur kann zu Overfitting führen, während eine flachere Struktur zu Underfitting führen kann. Wir verwenden hier die Werte None, 5 und 10. None bedeutet, dass es keine festgelegte maximale Tiefe gibt.\n",
    "- **Number of Estimators**: Hier wird die Anzahl der Bäume (Schätzer) im Ensemble festgelegt. Eine höhere Anzahl von Bäumen kann zu einer besseren Modellleistung führen, erfordert jedoch mehr Rechenressourcen. Wir legen die Werte 50, 100 und 200 zur Auswahl fest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of Estimators**: Hier wird die Anzahl der Bäume (Schätzer) im Ensemble festgelegt. Eine höhere Anzahl von Bäumen kann zu einer besseren Modellleistung führen, erfordert jedoch mehr Rechenressourcen. Wir legen die Werte 50, 100 und 200 zur Auswahl fest.\n",
    "- **Max Depth**: Max Depth meint die maximale Tiefe eines einzelnen Entscheidungsbaums. Eine tiefere Baumstruktur kann zu Overfitting führen, während eine flachere Struktur zu Underfitting führen kann. Wir verwenden hier die Werte None, 5 und 10. None bedeutet, dass es keine festgelegte maximale Tiefe gibt.\n",
    "- **Max Leaves**: Max Leaves meint die maximale Anzahl von Blättern in einem Baum und begrenzt das Wachstum der Bäume.\n",
    "- **Learning Rate**: Die Lernrate steuert, wie stark jeder Baum die vorherigen Bäume korrigiert. Eine niedrige Lernrate erfordert im Normalfall mehr Bäume, um den gleichen Effekt zu erzielen, kann jedoch im Gegenzug zu einer besseren Generalisierung führen. Hier werden Werte von 0.1 bis 0.5 in Schritten von 0.1 verwendet.\n",
    "- **Booster**: Hier wird die Art des Boosters, der verwendet wird, festgelegt. \"gbtree\" steht für Entscheidungsbäume, \"gblinear\" für lineare Modelle und \"dart\" für Dropout-gestützte Entscheidungsbäume.\n",
    "- **Grow Policy**: Die Grow Policy legt die Strategie für das Wachstum der Bäume fest. 0 steht für \"depthwise\", 1 für \"lossguide\". Bei \"depthwise\" wächst der Baum schichtweise, wohingegen \"lossguide\" eine strukturiertere Wachstumsrichtlinie basierend auf dem Verlust verwendet.\n",
    "- **Gamma**: Dieser Paramter gibt an, wie stark ein Baumzweig, basierend auf dem Verlustrückgang, geschnitten wird. Ein höherer Gamma-Wert führt zu konservativerem Pruning.\n",
    "- **Regularization Alpha**: Hiermit ist der L1 Regularisierungsterm auf den Gewichtungen der Blätter gemeint. Dieser hilft bei der Vermeidung von Overfitting durch Sparsamkeit.\n",
    "- **Regularization Lambda**: Dieser Parameter meint den L2 Regularisierungsterm auf den Gewichtungen der Blätter und hilft bei der Vermeidung von Overfitting durch Schrumpfung der Gewichtungen.\n",
    "- **Metric**: Metric meint die Bewertungsmetrik, die während des Trainings verwendet wird. Wir verwenden hier den Mean Absolute Error (skm.mean_absolute_error) als Bewertungsmetrik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ExtraTressRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of Estimators**: Hier wird die Anzahl der Bäume (Schätzer) im Ensemble festgelegt. Eine höhere Anzahl von Bäumen kann zu einer besseren Modellleistung führen, erfordert jedoch mehr Rechenressourcen. Wir legen die Werte 50, 100 und 200 zur Auswahl fest.\n",
    "- **Criterion**: Criterion wählt das Kriterium aus, das zur Auswahl der besten Aufteilung in jedem Entscheidungsbaumknoten verwendet wird ,z.B. squared_error (mittlerer quadratischer Fehler), absolute_error (mittlerer absoluter Fehler), friedman_mse (verbesserte MSE für Gradient Boosting) oder poisson (Poisson-Regression für Zählvariablen).\n",
    "- **Max Features**: Dieser Paramter definiert die maximale Anzahl der Merkmale, die für die Suche nach der besten Aufteilung in einem Baumknoten verwendet werden, z.B. sqrt (Quadratwurzel der Anzahl der Merkmale) oder log2 (Logarithmus zur Basis 2 der Anzahl der Merkmale):\n",
    "- **Max Depth**: Max Depth meint die maximale Tiefe eines einzelnen Entscheidungsbaums. Eine tiefere Baumstruktur kann zu Overfitting führen, während eine flachere Struktur zu Underfitting führen kann. Wir verwenden hier die Werte None, 5 und 10. None bedeutet, dass es keine festgelegte maximale Tiefe gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Auswahl der optimalen Hyperparamter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden alle Hyperparameter definiert. Die Listen enthalten die möglichen Werte eines jeden Parameters. Jedes Modell hat verschiedene Hyperarameter zur Auswahl, die bereits vorgestellt wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbRegressor = {\n",
    "    \"loss\": ['squared_error'],#, 'absolute_error', 'huber', 'quantile'],\n",
    "    \"alpha\": [0.2*x for x in range(1, 5)],\n",
    "    \"criterion\": ['friedman_mse', 'squared_error'],\n",
    "    \"learning_rate\": [0.1*x for x in range(-2, 3)],\n",
    "    \"max_depth\": [None, 5, 10],\n",
    "    \"n_estimators\": [50, 100, 200]\n",
    "}\n",
    "\n",
    "xgbRegressor = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 5, 10],\n",
    "    \"max_leaves\": [0, 5, 10, 20],\n",
    "    \"learning_rate\": [0.2*x for x in range(1, 5)],\n",
    "    \"booster\": [\"gbtree\", \"gblinear\", \"dart\"],\n",
    "    \"grow_policy\": [0, 1],\n",
    "    \"gamma\": [1e-3, 1e-1, 1],\n",
    "    \"reg_alpha\": [1e-3, 1e-1, 1],\n",
    "    \"reg_lambda\": [1e-3, 1e-1, 1],\n",
    "    \"metric\": [mean_absolute_error]\n",
    "}\n",
    "\n",
    "etRegressor = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"criterion\": [\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"max_depth\": [None, 3, 5, 10]\n",
    "}\n",
    "\n",
    "features = {\n",
    "    ske.GradientBoostingRegressor: gbRegressor,\n",
    "    xgb.XGBRegressor: xgbRegressor,\n",
    "    ske.ExtraTreesRegressor: etRegressor\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Hilfe von `GridSearchCV` wird ein Hyperparameter-Tuning für jedes Modell durchgeführt, um jeweils die beste Kombination von Hyperparametern zu finden. Hierbei verwenden wir den negativen Mean Absolute Error als Bewertungsmetrik. Eine automatische Kreuzvalidierung findet ebenfalls statt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select_results = pd.DataFrame(columns=[\"model\", \"params\", \"mae\"])\n",
    "\n",
    "\n",
    "for model_type in features:\n",
    "    gs_model = sms.GridSearchCV(estimator=model_type(random_state=1),\n",
    "                                param_grid=features[model_type],\n",
    "                                n_jobs=-1,\n",
    "                                scoring='neg_mean_absolute_error')\n",
    "\n",
    "    gs_model.fit(train_data.drop([\"cnt\"], axis=1), train_data[\"cnt\"])\n",
    "    print(f\"Model type {model_type.__name__}\\nreached MAE of {gs_model.best_score_*-1}\\n Params: {gs_model.best_params_}\")\n",
    "    model_select_results.loc[len(model_select_results)] = [type(gs_model.best_estimator_), gs_model.best_params_, gs_model.best_score_*-1]\n",
    "\n",
    "model_select_results.sort_values(\"mae\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell mit dem besten Ergebnis ist der `XGBRegressor`. Der MAE beträgt 670.95 für folgende Hyperparamter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select_results.at[1, \"params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion gibt alle möglichen, einzigartigen Kombinationen der unabhängigen Variablen aus. Die Liste enthält die höchste Anzahl an Features zu Beginn und wird danach immer kürzer (Backward Eliminiation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_combinations(features, length):\n",
    "    all_combinations = []\n",
    "    for r in range(1, length + 1):\n",
    "        combinations = itertools.combinations(features, r)\n",
    "        all_combinations.extend(combinations)\n",
    "\n",
    "    # Backwards Elimination\n",
    "    return all_combinations[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion nimmt eine Liste von Indexes auf und gibt die Liste der numerischen Indexes zurück. Dies wird benötigt, da die `numpy`-Arrays nach der Skalierung numerische Indexes verwenden. Um auf bestimmte Features zuzugreifen, wird der numerische Index gebraucht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion zur Evaluation einer Feature-Kombination mittels MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_params(features):\n",
    "    reg = Ridge(random_state=1)\n",
    "    reg.fit(train_data[features], train_data[\"cnt\"])\n",
    "\n",
    "    y_pred = reg.predict(validation_data[features])\n",
    "    mae = mean_absolute_error(validation_data[\"cnt\"], y_pred)\n",
    "    coefs = reg.coef_\n",
    "    reg\n",
    "\n",
    "    return dict(features=features,\n",
    "                coefs=coefs,\n",
    "                mae=mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorgehensweise:\n",
    "- Es wird über alle möglichen Feature-Kombinationen iteriert und ein `Ridge`-Regressionsmodell erstellt. `Ridge` wird verwendet, da die `LinearRegression` Probleme mit overfitting verursacht.\n",
    "- Die verwendeten Features, die Koeffizienten, sowie der MAE werden in einem DataFrame gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = len(features)\n",
    "feature_list = get_feature_combinations(features, num_of_features)\n",
    "c = 0\n",
    "print(\"Feature combinations\", len(feature_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insgesamt gibt es ca. 4,2 Mio. einzigartige Kombinationen der Features. Für jede Kombination wird ein grundlegendes Modell erstellt und dessen MAE, sowie die Parameterkombination gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results = [{\"mae\": 0}]\n",
    "num_of_features = len(features)\n",
    "feature_list = get_feature_combinations(features, num_of_features)\n",
    "c = 0\n",
    "print(\"Feature combinations\", len(feature_list))\n",
    "for feature_comb in feature_list:\n",
    "    feature_results.append(eval_model_params(feature_comb))\n",
    "\n",
    "    c+=1\n",
    "\n",
    "    if c % 500 == 0:\n",
    "        print(f\"Evaluating {len(feature_comb)}/{len(features)} feature combinations (No. {c}) - MAE: {feature_results[-1]['mae']}\", end=\"\\r\")\n",
    "\n",
    "feature_results.remove({\"mae\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results = pd.DataFrame(feature_results).sort_values(\"mae\", ignore_index=True).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die besten Ausführungsergebnisse speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_results.loc[:10000, :].to_csv(\"feature_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenschaften der besten Modelle, sortiert nach dem MAE. Jede Zeile in dem DataFrame ist das Ergebnis eines Modells mit einer einzigartigen Feature-Kombination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features des besten Modells, nach MAE gewertet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results.iat[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Koeffizienten den jeweiligen Features zuordnen zu können, erstellen wir dafür ein separates DataFrame. Die Koeffizienten sind deshalb wichtig, da sie aussagen, wie wichtig ein Feature für eine genaue Vorhersage ist. Da die Daten bereits skaliert sind, sind alle Werte im Bereich [-1;1] und es gibt keine Gewichtungsunterschiede zwischen den Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dicts = []\n",
    "\n",
    "for idx, row in feature_results.iterrows():\n",
    "    reg_dicts.append({key: value for key, value in zip(row[\"features\"], row[\"coefs\"])})\n",
    "coef_df = pd.DataFrame(data=reg_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durchschnittliche Koeffizienten der jeweiligen Features im `Ridge`-Regressionsmodell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erkenntnisse:\n",
    "- Die gefühlte Temperatur hat einen relativ hohen Einfluss auf die Kundenzahl\n",
    "- An Feiertagen sind ca. 100 Kunden weniger unterwegs\n",
    "- Werbeblätter haben einen negativen Einfluss (Vielleicht doch weglassen?)\n",
    "- Eine Preisreduktion hat nur wenig Einfluss auf die Kundenzahl\n",
    "- Im Winter gibt es weitaus weniger Kunden, sonst gibt es keinen signifikaten Unterschied zwischen den Jahreszeiten\n",
    "- Die Wetterbedingungen haben eine entsprechende Auswirkung\n",
    "- Von Montag bis Mittwoch gibt es täglich weniger Kunden als an anderen Wochentagen\n",
    "- Je stärker die Windgeschwindigkeit, desto weniger Fahrräder werden gemietet\n",
    "- An Werktagen sind ca. 130 Kunden mehr unterwegs, was darauf hindeutet, dass diese Kunden Arbeitspendler sind\n",
    "- Im Flgejahr gibt täglich 1000 Kunden mehr, als im Erstjahr (Kundenzuwachs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koeffiziententabelle speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coef_df.describe().round(2).T.sort_index().to_csv(\"coefficients.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(mtype, features, params):\n",
    "\n",
    "    fin_model = mtype(random_state=1, **params)\n",
    "\n",
    "    fin_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = fin_model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return [mtype.__name__, mae, r2, fin_model, params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_features = feature_results.iat[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_models = pd.DataFrame(columns=[\"modeltype\", \"mae\", \"r2\", \"model\", \"params\"])\n",
    "for idx, row in model_select_results.iterrows():\n",
    "    fin_models.loc[len(fin_models)] = build_model(row[\"model\"], fin_features, row[\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_models.loc[1, \"params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelltyp**\n",
    "- `sklearn.ensemble.GradientBoostingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter**\n",
    "- Alpha: `0.2`\n",
    "- Messkriterium: `friedman_mse`\n",
    "- Lernrate: `0.1`\n",
    "- Loss: `squared_error`\n",
    "- Maximale Baumtiefe: `5`\n",
    "- Anzahl Regressoren: `100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verwendete Features**\n",
    "- `yr`\n",
    "- `mnth`\n",
    "- `workingday`\n",
    "- `atemp`\n",
    "- `leaflets`\n",
    "- `weekday_4`\n",
    "- `weekday_5`\n",
    "- `weekday_6`\n",
    "- `season_1`\n",
    "- `season_4`\n",
    "- `weathersit_1`\n",
    "- `weathersit_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(fin_models.at[1, \"model\"], open(\"../models/final_model_optimized.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
