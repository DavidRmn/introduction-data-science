default:

preprocessing:
  pipeline:
    _CustomTransformer:
      - func: target_to_datetime
        module: "idstools._transformer"
        config:
          target: dteday
          format: "%d.%m.%Y"
      - func: impute_season
        module: "idstools._transformer"
        config:
          target: season
          date: dteday
      - func: negative_to_nan
        module: "idstools._transformer"
        config:
          target: windspeed
      - func: process_weekday
        module: "idstools._transformer"
        config:
          target: weekday
          date: dteday
      - func: remove_outliers
        module: "idstools._transformer"
        config:
          target: casual
    _OneHotEncoder:
      - target: season
        config:
          prefix: season
          dtype: "int"
          drop_first: true
      - target: weekday
        config:
          prefix: weekday
          dtype: "int"
          drop_first: true
      - target: weathersit
        config:
          prefix: weathersit
          dtype: "int"
          drop_first: true
    _NaNDropper:
      - target: ['windspeed']
    _SimpleImputer:
      - target: ['hum']
        config:
          strategy: "mean"
    _FeatureDropper:
      - target: ['casual', 'registered', 'temp', 'dteday']
        config:
          axis: 1
          errors: "ignore"

optimization:
  pipeline:
    - id: LazyRegressor
      targets: [1]
      model: LazyRegressor
      module: lazypredict.Supervised
      config:
        verbose: 0
        predictions: true
        ignore_warnings: false
        random_state: 42
      split:
        config:
          test_size: 0.2
          random_state: 42
      validation:
        targets: [2]

    - id: LinearRegression
      targets: [1]
      model: LinearRegression
      module: sklearn.linear_model
      split:
        config:
          test_size: 0.2
          random_state: 42
      validation:
        targets: [2]
        methods:
          - mae
          - r2_score

    - id: RandomForestRegressor
      model: RandomForestRegressor
      module: sklearn.ensemble
      param_grid:
        n_estimators: [100, 200, 300]
        max_depth: [10, 20, 30]
        min_samples_split: [2, 5, 10]
        min_samples_leaf: [1, 2, 4]
        bootstrap: [True, False]
        random_state: [42]

    - id: XGBRegressor
      model: XGBRegressor
      module: xgboost
      param_grid:
        n_estimators: [100, 200, 300]
        max_depth: [3, 4, 5]
        learning_rate: [0.1, 0.01, 0.001]
        random_state: [42]

    - id: ExtraTreesRegressor
      model: ExtraTreesRegressor
      module: sklearn.ensemble
      param_grid:
        n_estimators: [100, 200, 300]
        max_depth: [10, 20, 30]
        min_samples_split: [2, 5, 10]
        min_samples_leaf: [1, 2, 4]
        bootstrap: [True, False]
        random_state: [42]

    - id: LinearRegression
      model: LinearRegression
      module: sklearn.linear_model
      param_grid:
        fit_intercept: [true, false]
        copy_X: [true, false]
        positive: [true, false]

    - id: GridSearchCV
      targets: [1]
      model: GridSearchCV
      module: sklearn.model_selection
      estimators: [RandomForestRegressor, XGBRegressor, ExtraTreesRegressor]
      config:
        cv: 5
        n_jobs: -1
        scoring: 'r2'
      split:
        config:
          test_size: 0.2
          random_state: 42
      validation:
        targets: [2]